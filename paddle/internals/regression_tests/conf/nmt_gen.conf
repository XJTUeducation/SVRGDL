#!/usr/bin/env python
# Copyright (c) 2016 Baidu, Inc. All Rights Reserved

"""
An attention version of GRU Encoder-Decoder network. It consists a 
bidirectional GRU as an encoder and a decoder that emulates searching 
through a source sentence during decoding a translation. See Paper: 
Neural Machine Translation by Jointly Learning to Align and Translate.
"""

import os
import sys
import math
import paddle.trainer.recurrent_units as recurrent
from paddle.trainer_config_helpers.data_sources import *

generating = get_config_arg("generating", bool, False)
gen_trans_file = get_config_arg("gen_trans_file", str, None)
# for beam search
beam_size = get_config_arg("beam_size", int, 3)
max_frames = 250

dir = "./regression_tests/data/nmt_gen/"
src_lang_dict = dir + "fr.dict"
source_language_dict_dim = len(open(src_lang_dict,"r").readlines())

trg_lang_dict = dir + "en.dict"
target_language_dict_dim = len(open(trg_lang_dict,"r").readlines())

clipping_thd = 25
word_vec_dim = 512
latent_chain_dim = 512

# default parameters
default_initial_smart(True)

model_type("recurrent_nn")
src_dict = dict()
for line_count, line in enumerate(open(src_lang_dict, "r")):
    src_dict[line.strip()] = line_count
trg_dict = dict()
for line_count, line in enumerate(open(trg_lang_dict, "r")):
    trg_dict[line.strip()] = line_count

train_list = None if generating else dir + 'train.lst'
test_list = dir + 'gen.lst' if generating else dir + 'test.lst'
if generating:
    trg_dict = None

define_py_data_sources(train_list, test_list,
                       module = ["regression_tests.provider.nmt",
                                 "regression_tests.provider.nmt_test"],
                       obj = "process",
                       args = {"src_dict": src_dict,
                               "trg_dict": trg_dict},
                       train_async = True)
if generating:
    Settings(algorithm = "sgd", batch_size = 1, learning_rate = 0)
else:
    Settings(
        algorithm = "sgd",
        learning_method = "adam",
        learning_rate = 5e-4,
        do_average_in_cpu = True,
        learning_rate_decay_a = 0,
        learning_rate_decay_b = 0,
        ada_rou = 0.95,
        ada_epsilon = 1e-6,
        batch_size = 1,
        average_window = 0.5,
        max_average_window = 2500 * 2,
        num_batches_per_send_parameter = 1,
        num_batches_per_get_parameter = 1,
    )

#################################### network ###################################
def gru_encoder(name, size, input,clipping_thd):
    recurrent.GatedRecurrentLayerGroup(
       name = "encoder_forward",
       size = size,
       para_prefix="_encoder_forward",
       active_type = "tanh",
       gate_active_type = "sigmoid",
       inputs = [FullMatrixProjection(input)],
       error_clipping_threshold = clipping_thd,
       seq_reversed = False
    )

    recurrent.GatedRecurrentLayerGroup(
       name = "encoder_backward",
       size = latent_chain_dim,
       para_prefix="_encoder_backward",
       active_type = "tanh",
       gate_active_type = "sigmoid",
       inputs = [FullMatrixProjection(input)],
       error_clipping_threshold = clipping_thd,
       seq_reversed = True
    )

    Layer(
        inputs = [Input("encoder_forward"), Input("encoder_backward")],
        name = name,
        active_type = "",
        type = "concat"
    )

    Layer(
        name = "encoder_backward_last",
        type = "seqfirstins",
        active_type = "linear",
        bias = False,
        inputs = [Input("encoder_backward")]
    )

    MixedLayer(
        name = "encoder_last_projected",
        active_type = "tanh",
        size = latent_chain_dim,
        bias = False,
        inputs = FullMatrixProjection("encoder_backward_last"),
    )
    return "encoder_last_projected"

DataLayer(
    name = "source_language_word",
    size = source_language_dict_dim,
)

if generating:
    Inputs("source_language_word", "sent_id")

    Layer(
        name = "sent_id",
        type = "data",
        size = 1, #info for printer
    )

else:
    Inputs("source_language_word", "target_language_word",\
            "target_language_next_word")
    Outputs("output")

    DataLayer (
        name = "target_language_word",
        size = target_language_dict_dim,
    )

    DataLayer(
        name = "target_language_next_word",
        size = target_language_dict_dim,
    )

    MixedLayer(
        name = "target_embedding",
        size = word_vec_dim,
        active_type = "",
        bias = False,
        inputs = TableProjection("target_language_word",
            parameter_name = "_target_language_embedding"),
    )

#################################### encoder ###################################
MixedLayer(
    name = "source_embedding",
    size = word_vec_dim,
    active_type = "",
    bias = False,
    inputs = TableProjection("source_language_word",
                 parameter_name = "_source_language_embedding"),
)

encoder_name = "encoder"
encoder_input = "source_embedding"
decoder_init = gru_encoder(
    name = encoder_name,
    size = latent_chain_dim,
    input = encoder_input,
    clipping_thd = clipping_thd,
)

#################################### decoder ###################################
MixedLayer( #for attention
    name = "encoder_projected",
    active_type = "",
    size = latent_chain_dim,
    bias = False,
    inputs = FullMatrixProjection(encoder_name),
)

decoder_input = "target_embedding"
RecurrentLayerGroupBegin(
    name = "decoding_layer_group",
    in_links = [] if generating else [decoder_input],
    out_links = ["predict_word"] if generating else ["output"],
    generator = Generator(max_num_frames = max_frames,\
                        beam_size = beam_size,
                        num_results_per_sample = beam_size) if generating else None
)

decoder_state_memory = Memory(
    name = "decoder_state",
    boot_layer = decoder_init,
    boot_bias = False,
    size = latent_chain_dim,
    is_sequence = False
)

##################### attention model to learn alignment #######################
encoder_out_memory = Memory(
    name ="encoder_out_projected",
    size = latent_chain_dim,
    boot_layer = "encoder_projected",
    is_sequence = True)

Layer(
    name = "decoder_state_projected",
    type = "mixed",
    size = latent_chain_dim,
    active_type = "",
    bias = False,
    inputs = FullMatrixProjection(decoder_state_memory),
)

Layer(
    name = "expand_decoder_state_projected",
    type = "expand",
    trans_type="non-seq",
    inputs = ["decoder_state_projected", encoder_out_memory],
)

Layer(
    name = "attention_vecs",
    type = "mixed",
    size = latent_chain_dim,
    active_type = "tanh",
    bias = False,
    inputs = [IdentityProjection("expand_decoder_state_projected"),
              IdentityProjection(encoder_out_memory)]
)

Layer(
    name = "attention_weight",
    type = "mixed",
    size = 1,
    active_type = "sequence_softmax",
    bias = False,
    inputs = FullMatrixProjection("attention_vecs"),
)

Layer(
    name = "context_vectors",
    type = "scaling",
    inputs = ["attention_weight",
              encoder_out_memory],
    )

Layer( # reduce sequence
    name = "context",
    type = "average",
    average_strategy = "sum",
    inputs = ["context_vectors"],
    )

Layer(
    name = "encoder_out_projected",# read-only memory
    type = "mixed",
    size = latent_chain_dim,
    active_type = "",
    bias = False,
    inputs = IdentityProjection(encoder_out_memory),
)

if generating:
    predict_word_memory = Memory(
          name = "predict_word",
          size = target_language_dict_dim,
          boot_with_const_id = 0,
    )
    MixedLayer(
      name = "target_embedding",
      size = word_vec_dim,
      active_type = "",
      bias = False,
      inputs = TableProjection(predict_word_memory,
                               parameter_name = "_target_language_embedding"),
    )

MixedLayer(
    name = "decoder_input_projected",
    active_type = "",
    size = latent_chain_dim * 3,
    bias = False,
    inputs = [FullMatrixProjection("context"),
             FullMatrixProjection(decoder_input)],
)
decoder_input_proj = "decoder_input_projected"

recurrent.GatedRecurrentUnit(
   name = "decoder_state",
   size = latent_chain_dim,
   active_type = "tanh",
   gate_active_type = "sigmoid",
   inputs = IdentityProjection(decoder_input_proj),
   error_clipping_threshold = clipping_thd,
   out_memory = decoder_state_memory,
)

Layer(
    name = "output",
    type = "mixed",
    size = target_language_dict_dim,
    active_type = "softmax",
    bias = Bias(parameter_name = "_output.wbias"),
    inputs = FullMatrixProjection("decoder_state",
        parameter_name = "_output.w"),
)

if generating:
    Layer(
        name = "predict_word",
        type = "maxid",
        inputs = ["output"],
    )
    Layer(
        name = "eos_check",
        type = "eos_id",
        eos_id = 1,
        inputs = ["predict_word"],
    )

RecurrentLayerGroupEnd("decoding_layer_group")

if generating:
    Evaluator(
        name = "target_printer",
        type = "seq_text_printer",
        dict_file = trg_lang_dict,
        result_file = gen_trans_file,
        inputs = ["sent_id","predict_word"],
    )

else:
    Layer(
        name = "cost",
        type = "multi-class-cross-entropy",
        inputs = ["output", "target_language_next_word"],
    )

    Evaluator(
        name = "token_error_rate",
        type = "classification_error",
        inputs = ["output", "target_language_next_word"]
    )

